{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLPeasy Demo - OKCupid\n",
    "# Applied Machine Learning Days, Lausanne - 25 January 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo you will use NLPEasy to quickly setup a Pandas-based pipeline, enhanced with ML-Methods and pre-trained models (e.g. word embeddings, sentiment analysis). The results will then be saved in Elasticsearch and a Kibana dashboard is generated automatically to explore the texts and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example to completely work you need to have Python at least in Version 3.6 installed.\n",
    "Also you need to have install and start either\n",
    "\n",
    "- **Docker** <https://www.docker.com/get-started>, direct download links for\n",
    "    [Mac (DMG)](https://download.docker.com/mac/stable/Docker.dmg) and\n",
    "    [Windows (exe)](https://download.docker.com/win/stable/Docker%20for%20Windows%20Installer.exe).\n",
    "- **Elasticsearch** and **Kibana**:\n",
    "<https://www.elastic.co/downloads/> or\n",
    "<https://www.elastic.co/downloads/elasticsearch-oss> (pure Apache licensed version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally before the workshop, on the terminal or inside this notebook issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nlpeasy scikit-learn matplotlib\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last command downloads a spacy model for the english language -\n",
    "for the following you need to have at least it's `md` (=medium) version which has wordvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse today a dataset using NLPeasy package. We will also need pandas and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nlpeasy as ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Elastic Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to running elastic instance on your local machine. If this can't be found, it will start an Open Source stack on your docker.\n",
    "\n",
    "By specifying mount_volume_prefix your Elastic data is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elk = ne.connect_elastic(elk_version='7.5.0',\n",
    "                         mount_volume_prefix='./elastic-data-all/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If it is started on docker it will on the first time pull the images (1.3GB)!\n",
    "BTW, this function is not blocking, i.e. the servers might only be active couple of seconds later.\n",
    "Setting mountVolumePrefix=\"./elastic-data/\" would keep the data of elastic in your\n",
    "filesystems and then the data survives container restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'OK Cupid' dataset in this workshop (the anonymized legal version of it).\n",
    "Let's load the data and see what we have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc = pd.read_csv('https://github.com/rudeboybert/JSE_OkCupid/raw/master/profiles.csv.zip')\n",
    "okc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some categorical / continuous variables about the OK cupid customers:\n",
    "* body type\n",
    "* diet\n",
    "* education\n",
    "* relgion\n",
    "* sex\n",
    "* astro sign\n",
    "* preferences for alcohol, cigarettes, drugs\n",
    "* sexual orientation\n",
    "* relationship status\n",
    "* language\n",
    "* location\n",
    "\n",
    "\n",
    "But they also needed to answer some questions (textual data):\n",
    "\n",
    "* essay0- My self summary\n",
    "* essay1- What I’m doing with my life\n",
    "* essay2- I’m really good at\n",
    "* essay3- The first thing people usually notice about me\n",
    "* essay4- Favorite books, movies, show, music, and food\n",
    "* essay5- The six things I could never do without\n",
    "* essay6- I spend a lot of time thinking about\n",
    "* essay7- On a typical Friday night I am\n",
    "* essay8- The most private thing I am willing to admit\n",
    "* essay9- You should message me if...\n",
    "\n",
    "Source: https://github.com/rudeboybert/JSE_OkCupid/blob/master/okcupid_codebook.txt  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do some cleanup on the data first to make it ready for analysis.\n",
    "* Replace NaN values\n",
    "* Remove HTML tags\n",
    "* Height inch --> cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: replace NaN with '', replace '\\n' with ' ' \n",
    "okc = okc.replace(np.nan, '', regex=True)\n",
    "okc = okc.replace('\\n', ' ', regex=True)\n",
    "\n",
    "#2: remove html tags\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "fixcols = okc.columns[okc.columns.str.startswith('essay')]\n",
    "for col in fixcols:\n",
    "    okc[col] = [BeautifulSoup(text).get_text() for text in okc[col] ]\n",
    "    print(col + ' done.')\n",
    "\n",
    "okc['offspring'] = [BeautifulSoup(text).get_text() for text in okc['offspring'] ]\n",
    "okc['sign'] = [BeautifulSoup(text).get_text() for text in okc['sign'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert height in inches to cm\n",
    "okc['height_cm'] = round(pd.to_numeric(okc['height'])*2.54, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Lat/Long for location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kibana does not containt mappings of place names to geographical coordinates.\n",
    "We have provided a file to map coordinates to the city names so that we can plot it in Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv('city_coordinates.csv')\n",
    "okc = okc.merge(cities, how='left', left_on='location', right_on = 'city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc['all_free_text'] =   \\\n",
    "        okc['essay0'] + '\\n' + \\\n",
    "        okc['essay1'] + '\\n' + \\\n",
    "        okc['essay2'] + '\\n' + \\\n",
    "        okc['essay3'] + '\\n' + \\\n",
    "        okc['essay4'] + '\\n' + \\\n",
    "        okc['essay5'] + '\\n' + \\\n",
    "        okc['essay6'] + '\\n' + \\\n",
    "        okc['essay7'] + '\\n' + \\\n",
    "        okc['essay8'] + '\\n' + \\\n",
    "        okc['essay9']\n",
    "\n",
    "okc['geolocation'] = okc['lat'].astype(str) + ',' + okc['long'].astype(str) \n",
    "okc = okc.drop(columns = ['lat', 'long'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Let's construct some other features that could be interesting.\n",
    "Some of the categorical features are too verbose, and we want to make it shorter.\n",
    "E.g. from the pets column, we will summarise those that like dogs and those that have dogs into a 'likes_dogs' category, like so:\n",
    "\n",
    "The same we will do for the cat lovers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc['likes_dogs'] = 0\n",
    "okc.loc[okc['pets'].str.contains('likes dogs'), 'likes_dogs'] = 1\n",
    "okc.loc[okc['pets'].str.contains('has dogs'), 'likes_dogs'] = 1\n",
    "    \n",
    "okc['likes_cats'] = 0\n",
    "okc.loc[okc['pets'].str.contains('likes cats'), 'likes_cats'] = 1\n",
    "okc.loc[okc['pets'].str.contains('has cats'), 'likes_cats'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc[['pets', 'likes_dogs', 'likes_cats']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, languages are stuffed into one column so let's take them apart.\n",
    "And let's count how many languages they speak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split language column\n",
    "okc[['language0','language1','language2','language3','language4']] = okc['speaks'].str.split(pat=',',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc['count_languages'] = (okc[['language0', 'language1', 'language2','language3','language4']].notnull()).astype(int).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns 'religion' and 'sign' also indicated how much people care about the respective topic.\n",
    "Let's break it into a clean religion and a clean sign column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate religion\n",
    "okc[['religion_clean', 'religion_remainder']] = okc['religion'].str.split(pat='and|but',expand=True)\n",
    "# isolate astro sign\n",
    "okc[['sign_clean', 'sign_remainder']] = okc['sign'].str.split(pat='and|but',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc['sign_clean'] = okc['sign_clean'].str.strip()\n",
    "okc['sign_clean'] = okc['sign_clean'].replace('', np.nan)\n",
    "okc['sign_clean'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc['religion_clean'] = okc['religion_clean'].str.strip()\n",
    "okc['religion_clean'] = okc['religion_clean'].replace('', np.nan)\n",
    "okc['religion_clean'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with ethnicity, to get it separated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc[['eth0','eth1','eth2','eth3','eth4','eth5','eth6','eth7','eth8']] = okc['ethnicity'].str.split(pat=', ',expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allright, we should be all set now to start our analysis.\n",
    "So let's deep dive into NLPeasy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPeasy Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to construct pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ne.Pipeline(index='okc',\n",
    "                       text_cols=['all_free_text', 'essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9'],\n",
    "                       num_cols = ['age', 'height_cm', 'income'],\n",
    "                       tag_cols=['body_type', 'diet', 'drinks', 'drugs', 'education', 'ethnicity', 'pets',\n",
    "                                 'religion', 'sex', 'sign', 'smokes', 'speaks', 'status', 'religion_clean',\n",
    "                                'sign_clean', 'eth0', 'likes_cats', 'likes_dogs'],\n",
    "                       geopoint_cols = ['geolocation'], \n",
    "                       elk=elk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters here have some meening:\n",
    "* `index` will be the name of the Elasticsearch index (something like a Database name).\n",
    "* `textCols` here you can specify which columns of the dataframe are textual.\n",
    "* `numCols` will be used for histograms in Kibana.\n",
    "* `tagCols` will be used for barplots in Kibana.\n",
    "* `geoPointCols` can be used to draw maps in Kibana.\n",
    "* `elk` is the Elastic stack we connected to above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add stages in the pipeline - the spacy model takes ~30 secs to load.\n",
    "Here, we do Sentiment analysis and Spacy Enrichment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline += ne.VaderSentiment(['all_free_text'] , 'sentiment')\n",
    "pipeline += ne.VaderSentiment(['essay0'], 'sentiment_summary')\n",
    "pipeline += ne.VaderSentiment(['essay1'], 'sentiment_life')\n",
    "pipeline += ne.VaderSentiment(['essay2'], 'sentiment_goodat')\n",
    "pipeline += ne.VaderSentiment(['essay3'], 'sentiment_noticeabout')\n",
    "pipeline += ne.VaderSentiment(['essay4'], 'sentiment_favorites')\n",
    "pipeline += ne.VaderSentiment(['essay5'], 'sentiment_6things')\n",
    "pipeline += ne.VaderSentiment(['essay6'], 'sentiment_thinkingabout')\n",
    "pipeline += ne.VaderSentiment(['essay7'], 'sentiment_Fridaynight')\n",
    "pipeline += ne.VaderSentiment(['essay8'], 'sentiment_privatething')\n",
    "pipeline += ne.VaderSentiment(['essay9'], 'sentiment_messageme')\n",
    "pipeline += ne.SpacyEnrichment(nlp='en_core_web_md',\n",
    "                               cols=['all_free_text', 'essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9'],\n",
    "                               vec=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLPeasy pipelines can have as few or as many stages as you wish. Here we just use the following:\n",
    "* `VaderSentiment` is a nice rule-based sentiment prediction for english.\n",
    "* `Spacy` uses the `spacy` package together with it's language model - here the english medium (=md). We use it for a couple of enrichments, as you will see below. Please try to add your own ones! Here we activated `vec=True` (default is `False`) in order to use the Spacy-WordEmbeddings later for clustering.\n",
    "\n",
    "There are more stages in NLPeasy you can use (e.g. RegexTag Extraction, Splitting) or you can define your own functions there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run the pipeline - Spacy needs some time to process, so these ~60000 messages take about 100 minutes to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this for a subset of the data as it will take too long otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc_enriched = pipeline.process(okc[range(0,1000),:], write_elastic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Kibana Dashboard of all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.create_kibana_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Kibana in webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kib= elk.show_kibana(how=[\"jupyter\",\"print\",\"webbrowser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline._tictoc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kibana Screenshot](demo_kibana.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "filename = '{}_okc_enriched'.format(datetime.datetime.now())\n",
    "okc_enriched.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If elastic was started on Docker and you want to shutdown the servers issue:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ne.docker.stop_elastic_on_docker(\"nlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Warning*: If you didn't use a mountVolumePrefix when you started the servers, all the data in elastic and kibana will be lost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
